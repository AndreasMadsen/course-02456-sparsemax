%!TEX root = main.tex
\begin{abstract}

We investigate if the sparsmax transformation suggested in Martins et al. \cite{sparsemax} can be a suitable and practical replacement for the softmax transformation. This is done by both looking at its predictive abilities, but also how it can be implemented in practice. The implementation is done for Tensorflow \cite{tensorflow2015-whitepaper}, in particular we discuss how a parallel GPU implementation of sparsemax can be constructed and measure its computational performance compared to softmax. The sparsemax transformation is applied to both classification problems and in an attention model. We show that sparsmax have similar classification performance as softmax for classification problems and greatly improves the attention model performance.

\end{abstract}

% USE: http://www.ieee.org/documents/taxonomy_v101.pdf
\begin{keywords}
neural networks, regression analysis, quadratic programming, cost function
\end{keywords}
