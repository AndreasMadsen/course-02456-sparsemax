%!TEX root = main.tex
\begin{abstract}

We investigate if the sparsmax transformation sugested in Martins et al. \cite{sparsemax} can be a sutiable and practical replacement for the softmax transformation. This is done by both looking at its predictive abilities, but also how it can be implemented in practise. The implementation is done for tensorflow \cite{tensorflow2015-whitepaper}, in particular we discuss how a parallel GPU implementation of sparsemax can be constructed and messure its computational performance compared to softmax. The sparsemax transformation is applied to both classification problems and in an attention model. We show that sparsmax have similar classification performance as softmax for classification problems and greatly improves the attention model performance.

\end{abstract}

% USE: http://www.ieee.org/documents/taxonomy_v101.pdf
\begin{keywords}
neural networks, regression analysis, quadratic programming, cost function
\end{keywords}
