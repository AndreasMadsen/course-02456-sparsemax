%!TEX root = main.tex
\begin{abstract}

This report investigates if the "sparsemax" transformation suggested in Martins et al. \cite{sparsemax} can be a suitable and practical replacement for the softmax transformation. This is done by looking at both its predictive abilities, but also how it can be implemented in practice. The implementation is done in Tensorflow \cite{tensorflow2015-whitepaper}; In particular a parallel GPU implementation of sparsemax is constructed and its computational performance is compared to softmax. The sparsemax transformation is applied to classification problems and in an attention model for a RNN. It is shown that sparsmax has a similar classification performance as softmax for classification problems and greatly improves the attention model performance.

\end{abstract}

% USE: http://www.ieee.org/documents/taxonomy_v101.pdf
\begin{keywords}
neural networks, regression analysis, quadratic programming, cost function
\end{keywords}
