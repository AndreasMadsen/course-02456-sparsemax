\section{Conclusion}
\todo[inline]{This section needs to be updated.}

We have shown how to successfully implement sparsemax in TensorFlow using Python-Numpy, C++ and CUDA. The sparsemax transformation and its associated loss function have been presented and the algorithmic counter parts. Our experiments indicate that the sparsemax transformation performs on par (or sligthly worse in terms of computing performance) with our benchmark algorith softmax. Experiments in multi-class and multi-label classification as well as in an encoder-decoder model with a sparse attention mechanism have been carried out. As sparsity is often a desired feature, both due to biological considerations but also computing-wise, future research in the context of deep learning applications of sparsemax could prove to be desirable.
