\section{Discussion}
Our tensorflow implementation of sparsemax has shown that it is a suitable alternative for the popular softmax transformation. As the experiments indicate, it is on par with the softmax transformation in terms of computing performance and predictive ability.
It has shown to create sparse attention that improve the performance of the text to digits task. As such, the sparsemax transformation has shown to be interesting in terms of deep learning and attention modelling.

An interresting result is that the multinormal regression on Iris dataset performed much better with sparsemax than with softmax. A likely explaination for this, is that sparsemax creates a sligt bias for the probability being either 0 or 1, because the Iris has very few observations such a bias would help with prediction. This is similar to using a prior distribution in bayesian statistics. If this is the case is not certain, but could be analysed by varying the dataset size for a larger or synthetic dataset.

The attention model turned out to produce much better results than what we could have hoped for, which is very encouraging. A possible issue that could have arrised in theory, is if the attention vector becomes $\{0, \cdots, 0, 1, 0, \cdots 0\}$. In that case the gradient will also be zero, and it will infeasible to optimize the model. But as shown in Figure \ref{fig:sparsemax} this was not an issue. Note that this will never be a problem for the multinomial regression as the loss function is convex, only for $\mathrm{sparsemax}(\mathbf{z}) = \mathbf{q}$ will the gradient be zero.

In the attention experiment we also tried using sparsemax in the output layer, this resulted in very similar good results. However it had the added effect of being more numerically stable than softmax, because the loss function doesn't use $\log(\cdot)$. Numerical stablility is also an advantage of sparsemax that one should consider.

\subsection{Implementation}
Implementing sparsemax was no trivial task, espcially compared to softmax. The implementation here uses an algorithm similar to that covered in Helgason et al. \cite{Helgason1980} as this is a fairly simple algorithm and it is well suited for parallel implementation on the GPU.

The main challenge when implementing sparsemax on the GPU is the sorting algorithm. This is because tensorflow and CuDNN does not provide a sorting implementation, it has thus been necessary to define our own custom sorting function in TensorFlow. Parallel sorting algorithms on the GPU are very difficult to implement, thus it's difficult to get a high-performing implementation. Our implementation uses an odd-even sorting algorithm, which has $\mathcal{O}(K)$ complexity. A faster sorting algorithm like bitonic sort which has $\mathcal{O}(\log(K))$ complexity could be considered, however these algorithms are much more difficult to implement. We do not belive that the choice of sorting algorithm have a big performance impact in our case, as the experiments have fairly few labels, and dimensionality in the attention case. When there are few labels the logits fit within the L1 cache, thus the odd-even sort doesn't have to interact with the global GPU memory. 

For the CPU case the situation is much different, here sequential solution algorithms can easily be used. These have shown to be much more efficient than the naive sorting approach \cite{Liu2016}. An obvious choice is thus to use two completely different algorithms for the CPU and GPU implementations. This is not something we have done here, but in future work we would strongly recommend this.

The computational performance from Table \label{tab:timings}, showed as expected that our Sparsemax GPU implementation underperforms (significant) the Softmax implementation, but not notably. While the softmax is much simpler than sparsemax, it does involve a lot of exponential calculations, which are slow to compute on a GPU. Sparsemax only uses addition and multiplication. Note that these observations only hold for the MNIST dataset, but given the small size of the other datasets, that is likely also the only relevant dataset to look at.

For the smaller dataset the Tensorflow CPU implementation is the fastest. This is likely because the TensorFlow Numpy version has a communication overhead, between the Tensorflow C++ backend and the Python frontend. Similarly the GPU version has an overhead from transfering data and starting kernels, particularly the latter would be an issue for a small dataset like Iris.

