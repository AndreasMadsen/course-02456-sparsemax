\section{Discussion}
The sparsemax implementation in tensorflow has shown that it is a suitable alternative for the popular softmax transformation. As the experiments indicate it is on par with the softmax transformation in terms of computing performance and the predictive ability. It further enhances the ability to interpret output probabilites compared to softmax. As such, the sparsemax transformation has shown to be interesting in terms of deep learning. It does, however, come with some difficulties. Since the transformation requires a sorting algorithm \footnote{Not neccessarily, but our implementation does.} it has been necessary to define our own custom sorting operations in TensorFlow. This  may be a limitation in other software, where such operations cannot easily be defined. Although we have successfully been able to parallelize the sorting algorithm, it is usually not considered a trivial task to implement such on the GPU and it may as such prove difficult to get a high-performing implementation. As our implementation implements an odd-even sort a faster sorting algorithm like bitonic sort could be considered. As another way to compute sparse probabilities, one could optimize a threshold value and then truncate all values below this threshold value to zero in the softmax transformation. It may prove easier to implement such a feature. Further experiments are required to conclude whether this has the same appeal as sparsemax.