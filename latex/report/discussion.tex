\section{Discussion}
Our tensorflow implementation of sparsemax has shown that it is a suitable alternative for the popular softmax transformation. As the experiments indicate it is on par with the softmax transformation in terms of computing performance and the predictive ability.
It has shown create sparse attention that improve the performance of the text to digits task. As such, the sparsemax transformation has shown to be interesting in terms of deep learning and attention modelling.

Implementing sparsemax is no trivial task, espcially compared to softmax. The implementation here uses an algorithm similar to that covered in Helgason et al. \cite{Helgason1980} as this is a fairly simple algorithm and it is well suited for parallel implementation on the GPU.

The main challenge when implementing sparsemax on the GPU is the sorting algorithm. This is because tensorflow and CuDNN does not provide a sorting implementation, it has thus been necessary to define our own custom sorting operations in TensorFlow. Parallel sorting algorithms are very difficult to implement in parallel on the GPU and it may as such prove difficult to get a high-performing implementation. Our implementation uses an odd-even sorting algorithm, which has $\mathcal{O}(n)$ complexity. A faster sorting algorithm like bitonic sort which has $\mathcal{O}(\log(n))$ complexity could be considered, however these algorithms are much more difficult to implement. We do not belive that the choice of sorting algorithm have a big performance impact in our case, as the experiements have fairly few labels and dimentionality in the attention case. When there are few labels the logits fits within the L1 cache and then the odd-even sort doesn't have to interact with the global GPU memory. 

For the CPU case the situation is much different, here sequential solution algorithms can easily be used. These have shown to much more efficinent that the na√Øve sorting appoach \cite{Liu2016}. An obiuse choices is thus to use two completly diffrent solution algorithms for the CPU and GPU implementations. This is not something we have done here, but in future work we would strongly recomend this.

Another way to compute sparse probabilities, is to optimize a threshold value and then truncate all values below this threshold value to zero in the softmax transformation. It may prove easier to implement such a feature. Further experiments are required to conclude whether this has the same appeal as sparsemax.

\todo[inline]{Iris label and computational performance, why}

The TensorFlow Numpy version has a cmmunication overhead between the Tensorflow C++ backend and the Python fontend, while the GPU version have an overhead in transfering data and starting kernels, particaully the latter would be an issue a small dataset like Iris.

\todo[inline]{numerically stability and gradient in attention.}