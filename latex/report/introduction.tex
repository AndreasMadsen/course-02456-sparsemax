\section{Introduction}

The softmax transformation is a well known component in classic statistics such as multinomial logistic regression. The transformation is also used in neural networks both in multi-class classification and more recently in attention mechanisms.

The softmax transformation is very popular, likely because of its simplicity. It is easy to evaluate, differentiate and has a simple convex loss function.

However, while softmax is very simple, its simplistic design is also what causes its shortcomings. It is impossible for its output to be sparse. This can be a big issue in cases where one would expect or want a sparse distribution.

A simple way of obtaining sparse probabilities, is to optimize a threshold value and then truncate all the softmax values below this threshold value to zero. This would be easy to implement, but this doesn't fit well with the cross entropy function that uses $\log(\mathrm{softmax}(\mathbf{z})_i)$ as it is not defined for $\log(0)$.

A recent alternative to softmax is the sparsemax transformation \cite{sparsemax}. This has been shown to have many properties in common with the softmax transformation. It also has an associated convex loss function. This can make it an attractive alternative to multinomial logistic regression, especially when the target is multi-label.

Another suitable application of sparsemax is in attention mechanisms. When a standard Encoder-Decoder RNN is applied to a sentence the last state of the encoder has to retain all the information to generate the decoding. For longer sentences this requires a huge state vector which is hard to learn. Attention mechanisms let's the decoder selectively look at different parts of the sentence based on what the encoding is and decoding produced so far. This reduces the needed size of the encoding state vector. This attention mechanisms is ecentially a weighed mean, where the weights traditionally are calculated using a sparsemax transformation. However in many application such as natural language processing it could make sense for these weights to be sparse, such the attention is sparse.

In this paper we will show if:
\begin{itemize}
\item Sparsemax is useful as a multinomial logistic regression.
\item Sparsemax is useful in attention models and if the attention becomes sparse.
\item Sparsemax is computationally equviliant to softmax.
\end{itemize}
