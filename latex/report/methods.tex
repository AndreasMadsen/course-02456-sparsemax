\section{Methods}

The softmax transformation is defined as:
\begin{equation}
\mathrm{softmax}_i(\mathbf{z}) \coloneqq \frac{\exp(z_i)}{\sum_{j} \exp(z_j)}
\end{equation}

This transformation maps from $z \in \mathbb{R}^K$ to a probability distribution $\mathbb{P}^K \coloneqq \{ \mathbf{p} \in \mathbb{R}^K,  \mathbf{1}^T \mathbf{p} = 1, \mathbf{p} \ge \mathbf{0} \}$. $\mathbf{1}^T \mathbf{p} = 1$ is ensured by dividing by the sum, likewise $\mathbf{p} \ge \mathbf{0}$ is ensured by the exponential function. However it is precisely the exponential function that prevents the softmax properbility distribution from becoming sparse.

To allow sparsity, Martins et al. \cite{sparsemax} suggest  using the following optimization problem instead:
\begin{equation}
\mathrm{sparsemax}(\mathbf{z}) \coloneqq \argmin_{\mathbf{p}\, \in\, \mathbb{P}^K} \left\lVert \mathbf{p} - \mathbf{z} \right\rVert_2^2 
\end{equation}

This can be written as a constrained quadratic program:
\begin{equation}
\begin{aligned}
\min_{\mathbf{p}}\ &\mathbf{p}^T \mathbf{p} - 2 \mathbf{z}^T \mathbf{p} \\
\text{s.t. } &\mathbf{1}^T \mathbf{p} = 1 \\
&\quad\ \mathbf{p} \ge \mathbf{0}
\end{aligned}
\label{eq:sparsemax-QP}
\end{equation}

\subsection{Sparsemax}
Problem \eqref{eq:sparsemax-QP} is the constrained quadratic program known as the ``continuous quadratic knapsack problem'' or a ``singly linearly constrained quadratic program''. Solving it has been studied for a few decades. The breakthrough happened in 1980 where Helgason et al. \cite{Helgason1980} proposed an $\mathcal{O}(K \log(K))$ algorithm for solving it.
\begin{algorithm}[H]
  \caption{Calculate sparsemax probability distribution from logits $\mathbf{z}$.}
  \begin{algorithmic}[1]
    \Function{Sparsemax}{$\mathbf{z}$}
      \State \Call{Sort}{$\mathbf{z}$} as $z_{(1)} \ge \cdots \ge z_{(K)}$
      \Let{$k(\mathbf{z})$}{$\max \left\{ k \in [1 .. K] \mid 1 + k z_{(k)} > \sum_{j \le k} z_{(j)} \right\}$}
      \Let{$\tau(\mathbf{z})$}{$\frac{\left(\sum_{j \le k(\mathbf{z})} z_{(j)}\right) - 1}{k(\mathbf{z})}$}
      \State \Return{$[\max(z_1 - \tau(\mathbf{z}), 0), \cdots, \max(z_K - \tau(\mathbf{z}), 0)]$}
    \EndFunction
  \end{algorithmic}
\end{algorithm}

Other solution methods has then later been developed, \cite{Liu2016} contains a short overview of these. A popular approach is based on median search, these algorithms have a linear time complexity. The first algorithm using median search is \cite{brucker1984a}, for a general overview of more recent algorithms see \cite{kiwiel2008a-median}. Another category is the variable fixing method, these methods avoids sorting and median finding, by finding the solution to one variable in each iteration. These algorithms have quadratic complexity, but performs much better in practice \cite{kiwiel2008a-fixing}.

Unfortunately all of these methods were developed with serial computation in mind. As a consequence, they do not easily apply to a GPU implementation, which has become the mainstream computational device for neural networks. For this reason the original method \cite{Helgason1980} has been used, with only minor modifications for parallelism as described in Algorithm \ref{sparsemax-parallel-algorithm}.

\begin{algorithm}[H]
  \caption{Parallel sparemax.}
  \begin{algorithmic}[1]
    \Function{Sparsemax}{$\mathbf{z}$}
      \Let{$\mathbf{s}$}{\Call{Sort}{$\mathbf{z}$}} \Comment{For example bitonic sort}
      \Let{$\mathbf{c}$}{\Call{Cumsum}{$\mathbf{s}$}} \Comment{Parallel prefix sum}
      \Let{$\mathbf{b}$}{\Call{Map}{$\lambda (k) \coloneqq 1 + k s_k > c_k$, $k \in [1..K]$}}
      \Let{$k(\mathbf{z})$}{\Call{Sum}{$\mathbf{b}$}} \Comment{Parallel reduce}
      \Let{$\tau(\mathbf{z})$}{$\frac{c_{k(\mathbf{z})} - 1}{k(\mathbf{z})}$}
      \State \Return{\Call{Map}{$\lambda (k) \coloneqq \max(z_k - \tau(\mathbf{z}), 0)$, $k \in [1..K]$}}
    \EndFunction
  \end{algorithmic}
  \label{sparsemax-parallel-algorithm}
\end{algorithm}

Assuming $K$ processors Algorithm \ref{sparsemax-parallel-algorithm} has time complexity $\mathcal{O}(\log(K))$, which is the same as softmax.

In neural network optimization where batches or mini-batches are used, one can also parallelize over the observations.

The sparsemax transformation is mostly differentiable, resulting in a well-defined Jacobian for the transformation. In Tensorflow \cite{tensorflow2015-whitepaper} the chain rule is used for automatic differentiation. For example if the logits $\mathbf{z}$ is a function of some weight matrix $\mathbf{W}$, then we have:
\begin{equation}
\frac{\mathrm{sparsemax}(\mathbf{z})}{\partial \mathbf{W}} = \frac{\mathrm{sparsemax}(\mathbf{z})}{\partial \mathbf{z}} \frac{\partial \mathbf{z}}{\partial \mathbf{W}}
\end{equation}

Thus in Tensorflow it is the ``Jacobian times a vector'' operation that needs to be defined. From \cite{sparsemax} this is:
\begin{equation}
\mathbf{J}_{\mathrm{sparsemax}}(\mathbf{z}) \cdot \mathbf{v} = \mathbf{s} \odot (\mathbf{v} - \hat{v} \mathbf{1}), \text{  where } \hat{v} \coloneqq \frac{\mathbf{s}^T \mathbf{v}}{||\mathbf{s}||}
\end{equation}

\subsection{Sparsemax Loss}

The sparsemax loss function is derived by defining the gradient of the sparsemax loss to be similar to the gradient of the softmax loss (entropy loss):
\begin{equation}
\nabla_{\mathbf{z}} \mathcal{L}_{\mathrm{sparsemax}}(\mathbf{z}; \mathbf{q}) = -\mathbf{q} + \mathrm{sparsemax}(\mathbf{z})
\label{sparsemax-loss-gradient}
\end{equation}
where $\mathbf{q} \in \mathbb{P}^K$ is the target probability distribution.

From the gradient \eqref{sparsemax-loss-gradient} one can easily create the ``Gradient times a scalar'' operation that tensorflow needs.
\begin{equation}
\nabla_{\mathbf{z}} \mathcal{L}_{\mathrm{sparsemax}}(\mathbf{z}; \mathbf{q}) \cdot v = (-\mathbf{q} + \mathrm{sparsemax}(\mathbf{z})) v
\end{equation}

From \eqref{sparsemax-loss-gradient}, its primitive function, the sparsemax loss can the be derived \cite{sparsemax}:
\begin{equation}
\mathcal{L}_{\mathrm{sparsemax}}(\mathbf{z}; \mathbf{q}) = \frac{1}{2} \sum_{j \in S(\mathbf{z})} (z_j^2 - \tau^2(\mathbf{z})) + \frac{1}{2} ||\mathbf{q}||^2 - \mathbf{q}^T \mathbf{z}
\label{sparsemax-loss-original}
\end{equation}

The loss function \eqref{sparsemax-loss-original} dependency on $\tau(\mathbf{z})$ is not ideal, as it is inconvenient to recalculate it or alternatively store it in memory. Fortunately, because the sum is over the support $S(\mathbf{z})$ one can reformulate it to only be dependent on $\mathbf{z}$, $\mathrm{sparsemax}(\mathbf{z})$ and $\mathbf{q}$:
\begin{equation}
\mathcal{L}_{\mathrm{sparsemax}}(\mathbf{z}; \mathbf{q}) = \frac{1}{2} \sum_{j \in S(\mathbf{z})} p_j (2 z_j - p_j) + \frac{1}{2} ||\mathbf{q}||_2^2 - \mathbf{q}^T \mathbf{z}
\label{sparsemax-loss-reformulated}
\end{equation}
where $\mathbf{p} = \mathrm{sparsemax}(\mathbf{z})$. Finally the sparsemax loss \eqref{sparsemax-loss-reformulated} can be changed slightly, to be a simple map and reduce which is ideal for parallization: 
\begin{equation}
\mathcal{L}_{\mathrm{sparsemax}}(\mathbf{z}; \mathbf{q}) = \mathbf{1}^T \left( \mathbf{s} \odot \mathbf{p} \odot \left(\mathbf{z} - \tfrac{1}{2} \mathbf{p}\right) + \mathbf{q} \odot \left(\tfrac{1}{2} \mathbf{q} - \mathbf{z}\right)\right)
\end{equation}

\subsection{Sparsemax Regression}
We define Sparsemax regression as the equivalent to a multivariate logistic regression, but where the activation function at the output layer is sparsemax rather than softmax and with a L2-regularization on the weights and biases. Thus the minimization problem becomes
\begin{equation}
\min_{\mathbf{W},\mathbf{b}} \, \frac{\lambda}{2} (||\mathbf{W}||^2_{F} + ||\mathbf{b}||^2_{F}) + \frac{1}{N} \sum\limits_{i=1}^n \mathcal{L} (\mathbf{W} \mathbf{x}_i + \mathbf{b}; y_i)
\label{sparsemax-regression}
\end{equation}
Where $\mathcal{L} = \mathcal{L}_{\mathrm{sparsemax}}$ defined by \eqref{sparsemax-loss-original} and \eqref{sparsemax-loss-reformulated}.

\subsection{Sparsemax Attention}
A suitable application of sparsemax is in attention mechanisms. When a standard Encoder-Decoder RNN is applied to a sentence the last state of the encoder has to retain all the information to generate the decoding. For longer sentences this requires a huge state vector which is hard to learn. Attention mechanisms let's the decoder selectively look at different parts of the sentence based on what the encoding is and decoding produced so far. This reduces the needed size of the encoding state vector.

For encoding-decoding task the classical attention approach is to use a softmax activation function to compute the attentions ${\boldsymbol\alpha}_i$. This means that ${\boldsymbol\alpha}_i=0$ never occurs, which implies that the decoder does pay some attention to the entire sequence. However, if one uses the sparsemax activation function ${\boldsymbol\alpha}_i$ will be sparse and thus the model should only "attend" to the relevant subset of the sequence.

The model used here is a naive simplification of the one presented in the Bahdanau et al., 2014 model \cite{attention}.

\begin{equation}
\begin{aligned}
\text{encoding:} & \\
& \mathbf{h}_t = \mathrm{GRU}(\overrightarrow{\mathbf{E}} \mathbf{x}_t, \mathbf{h}_{t-1}) \\
\text{attention:} & \\
& e_{ij} = \mathbf{v}^T \tanh(\mathbf{W} \mathbf{s}_{i-1} + \mathbf{U} \mathbf{h}_j) \\
& {\boldsymbol\alpha}_i = \mathrm{sparsemax}(\mathbf{e}_i) \\
& \mathbf{c}_i = {\textstyle \sum_{j=1}^T} \alpha_{ij} \mathbf{h}_j \\
\text{decoder:} & \\
&\mathbf{s}_i = \mathrm{GRU}(\mathbf{c}_i, \mathbf{s}_{i-1}) \\
&\mathbf{y}_i = \mathrm{softmax}(\overleftarrow{\mathbf{E}} \mathbf{s}_i)
\end{aligned}
\label{eq:attention-model}
\end{equation}

In this attention model \eqref{eq:attention-model}, $\overrightarrow{\mathbf{E}}$ and $\overleftarrow{\mathbf{E}}$ are embedding models, trained simultaniusely with the RNN. $\mathbf{W}$, $\mathbf{U}$ are weight matrices and $\mathbf{v}$ is a weight vector.
