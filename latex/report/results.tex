\section{Results}
The sparsemax transformation is evaluated using the model defined in \eqref{sparsemax-regression} in terms of both classification performance and computational performance. It is also used in attention modelling as described in \eqref{eq:attention-model}. For comparison the results using softmax are also reported.

\subsection{Label estimation}
Five well-known benchmark datasets are used, as shown in Table \ref{tab:datasets}. The digit dataset MNIST and flower dataset Iris are multi-class classification, while the Scene, Emotions and CAL500 datasets are multi-label classification.\footnote{In multi-class classification the classes are mutually exclusive. In the multi-label case several labels can be associated with one observation.} 
\begin{table}[H]
\centering
\input{tables/descriptions.tex}
\caption{Summary for the five benchmark datasets used.}
\label{tab:datasets}
\end{table}
For comparison of performance the Jensen-Shannon divergence between the predicted distribution and target distribution is reported.
\begin{equation*}
\mathbf{JS(q,p)}:=\frac{1}{2}\mathbf{KL}\left(\mathbf{q}\Big|\Big|\frac{\mathbf{q}+\mathbf{p}}{2}\right)+\frac{1}{2}\mathbf{KL}\left(\mathbf{p}\Big|\Big|\frac{\mathbf{q}+\mathbf{p}}{2}\right)
\end{equation*}

Were $\mathbf{p}, \mathbf{q}$ are the predicted and target distributions, respectively and $\mathbf{KL}$ is the Kullbach-Leibler distance. For both the classifiers an L2 regularizer is used. The optimized regularization parameters can be seen in Table \ref{tab:hyperparameters}. The hyperparameter was tuned using a stratified 5-split cross-validation for the MNIST and Iris datasets and non-stratified for the rest. The resulting experiments are shown in Figure \ref{fig:hyperparameters}. The models were optimized using the Adam algorithm with default parameters, $\lambda = 0.001, \beta_1=0.9, \beta_2 = 0.999, \epsilon = 10^{-8}$.

\begin{table}[H]
\centering
\input{tables/hyperparameter.tex}
\caption{Regularization values and the corresponding JS divergence for both classifiers. 5-fold cross validation was used on the training data. The 95\% confidence interval is shown.}
\label{tab:hyperparameters}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{figures/hyperparameter.pdf}
\caption{JS divergence using a range of regularizers on all five datasets for the two regressors. The 95\% confidence interval is shown.}
\label{fig:hyperparameters}
\end{figure}

The JS divergence on the test set for the five datasets and the error rate for the multi-class datasets is shown in Table \ref{tab:results}. The softmax and sparsemax classifiers are very much alike in terms of performance. The Iris dataset shows an improved performance, which is also apparent in the regularization optimization (Table \ref{fig:hyperparameters}) where a statistical significant difference exists for all parameter choices.

\begin{table}[H]
\centering
\input{tables/classification.tex}
\caption{JS divergence for the five benchmark datasets and the sparsemax classifier as well as the softmax classifier.}
\label{tab:results}
\end{table}

\begin{table*}
\centering
\input{tables/timings.tex}
\caption{Time in seconds with associated confidence intervals.}
\label{tab:timings}
\end{table*}

\subsection{Computational performance}
Four implementations were benchmarked using the datasets from Table \ref{tab:datasets}.
\begin{itemize}
\setlength\itemsep{-0.3em}
\item Softmax: A TensorFlow implementation of Softmax Regression.
\item Numpy: A pure Numpy implementation of Sparsemax Regression.
\item TF Numpy: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented using only Numpy.
\item TF CPU: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented in C++ for the CPU.
\item TF GPU: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented in C++ and CUDA for the GPU.
\end{itemize}

In Table \ref{tab:timings} the computational performance is shown and compared with the native TensorFlow Softmax implementation. Each implementation has been run for 100 epochs. The GPU is an Nvidia\textsuperscript{\textregistered} Tesla K40c and the CPU is an Intel\textsuperscript{\textregistered} Xeon\textsuperscript{\textregistered} CPU E5-2630 v2 (12 cores).

For the large dataset, MNIST, the GPU implementation significantly outperforms all other sparsemax implementations as expected. The Numpy implementation runs significantly slower than the three TensorFlow implementations, also as expected. Compared to softmax, the GPU version of TensorFlow is significantly slower, but not by much.

For the Iris dataset, which is considerably smaller in size, the Numpy and TensorFlow CPU version outperforms both the GPU and Tensorflow Numpy version.

\subsection{Encoder-decoder with a sparse attention mechanism}
The encoder-decoder model defined in \eqref{eq:attention-model} has been tested on a synthetic translation dataset. The input sequence is a sequence of numbers spelled by regular characters e.g. ``three seven four''. The goal is then to translate this sequence to actual numbers e.g. ``374\#''. Where \# is the end of sequence tag.

The weights in the model have all been initialized using a truncated random normal with a standard deviation of 0.1, except for bias terms which have been initialized using a constant initializer of 0.

The model was optimized using the ADAM optimizer with a learning rate of 0.005, $\beta_1=0.9, \, \beta_2 = 0.999, \, \epsilon = 1\mathrm{e}{-8}$, batch sizes of 100 observations and for a 1000 epochs.

The results of applying sparse attention compared to soft attention, can be seen in the attention plots Figure \ref{fig:sparsemax} and Figure \ref{fig:softmax}. The validation accuracy is stated in the figure caption.

\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5,trim={5mm 12mm 5mm 5mm},clip]{figures/attention_softmax.pdf}
	\caption{Softmax attention for an Encoder-Decoder RNN. Validation accuracy after 100k training examples: $\approx 75$\%.}
	\label{fig:softmax}
\end{figure}
\begin{figure}[H]
 	\centering
 	\includegraphics[scale=0.5,trim={5mm 12mm 5mm 5mm},clip]{figures/attention_sparsemax.pdf}
 	\caption{Sparsemax attention for an Encoder-Decoder RNN. Validation accuracy after 100k training examples: $\approx 98$\%.}
 	\label{fig:sparsemax}
\end{figure}
  
