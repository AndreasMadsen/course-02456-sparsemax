\section{Results}
Next the sparsemax transformation is evaluated using the model defined by \eqref{sparsemax-regression}. For comparison we report the softmax regression results as well. The four different implementations plus the reference softmax implementation are referred to as:
\begin{itemize}
\setlength\itemsep{-0.3em}
\item Softmax: A TensorFlow implementation of Softmax Regression.
\item Numpy: A pure Numpy implementation of Sparsemax Regression.
\item TF Numpy: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented using only Numpy.
\item TF CPU: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented in C++ for the CPU.
\item TF GPU: A TensorFlow implementation where the custom ops associated with Sparsemax have been implemented in C++ and CUDA for the GPU.
\end{itemize}
For the sake of time performance comparison all five implementations are used. For the performance only the TensorFlow C++ version of Sparsemax and Softmax are used, as the results are the same for all Sparsemax implementations.

\subsection{Label estimation}
Five well-known benchmark datasets are used, as shown in Table \ref{tab:datasets}. The digit dataset MNIST and flower dataset Iris are multi-class classification, while the Scene, Emotions and CAL500 datasets are multi-label classification.\footnote{In multi-class classification the classes are mutually exclusive, while in the multi-label case several labels can be associated with one observation reflecting a relation between the labels.} 
\begin{table}[H]
\centering
\input{tables/descriptions.tex}
\caption{Summary for the five benchmark datasets used.}
\label{tab:datasets}
\end{table}
For comparison of performance the Jensen-Shannon divergence between the predicted and target distributions is reported.
\begin{equation*}
\mathbf{JS(q,p)}:=\frac{1}{2}\mathbf{KL}\left(\mathbf{q}\Big|\Big|\frac{\mathbf{q}+\mathbf{p}}{2}\right)+\frac{1}{2}\mathbf{KL}\left(\mathbf{p}\Big|\Big|\frac{\mathbf{q}+\mathbf{p}}{2}\right)
\end{equation*}

Where $\mathbf{p}, \mathbf{q}$ are the predicted and target distributions, respectively and $\mathbf{KL}$ is the Kullbach-Leibler distance. For all the classifiers an L2 regularizer is used and the regularization constants can be seen in Table \ref{tab:hyperparameters}. The hyperparameter was tuned using a stratified 5-split cross-validation for the MNIST and Iris datasets and non-stratified for the rest, where the resulting experiments are shown in Figure \ref{fig:hyperparameters}. The models where optimized using the Adam algorithm with default parameters, a learning rate of 0.001 and $\beta_1=0.9, \, \beta_2 = 0.999, \, \epsilon = 1\mathrm{e}{-8}$ and the Numpy implementation used Gradient Descent with a learning rate of $1\mathrm{e}{-2}$.

\begin{table}[H]
\centering
\input{tables/hyperparameter.tex}
\caption{Regularization values and the corresponding JS divergence for both classifiers. 5-fold cross validation was used in the training data.}
\label{tab:hyperparameters}
\end{table}
\begin{figure}[H]
	\centering
	\includegraphics[scale=1]{figures/hyperparameter.pdf}
\caption{JS divergence using a range of regularizers on all five datasets for the two regressors.}
\label{fig:hyperparameters}.
\end{figure}

The JS divergence on the test set for the five datasets and the miss rate for the multi-class datasets is shown in Table \ref{tab:results}. The Softmax and Sparsemax classifier are very much alike in terms of performance. The Iris dataset shows an improved performance, comparing the two best cases in the regularization optimization (Table \ref{tab:hyperparameters}), also shows a statistical significant difference.

\begin{table}[H]
\centering
\input{tables/results.tex}
\caption{JS divergence for the five benchmark datasets and the Sparsemax Classifier as well as the Softmax classifier.}
\label{tab:results}
\end{table}

\begin{table*}
\centering
\input{tables/timings.tex}
\caption{Time in seconds with associated confidence intervals.}
\label{tab:timings}
\end{table*}

\subsection{Computational performance}
The four implementations were benchmarked using the datasets from Table \ref{tab:datasets}. In Table \ref{tab:timings} the the time performance is shown and compared with the native TensorFlow Softmax implementation. Each implementation has been run for 100 epochs during timing on a Tesla K40 for the GPU implementations.

For the large dataset, MNIST, the GPU implementation significantly outperforms all other Sparsemax implementations as expected. The Numpy implementation runs significantly slower than the three TensorFlow implementations, also as expected. Compared to the Softmax, the GPU version of TensorFlow is significantly slower, but not by much.

For the Iris dataset, which is considerably smaller in size, the Numpy and TensorFlow CPU version outperforms both the GPU and Tensorflow Numpy version.

\subsection{Encoder-decoder with a sparse attention mechanism}
\subsubsection{Training procedure}
Weights in the encoder-decoder model defined in \eqref{eq:attention-model} have all been initialized using a truncated random normal with a standard deviation of 0.1 except for bias terms which have been initialized using a constant initializer of 1. 

The model was optimized using the ADAM optimizer with a learning rate of 0.005, $\beta_1=0.9, \, \beta_2 = 0.999, \, \epsilon = 1\mathrm{e}{-8}$, batch sizes of 100 observations and for a 1000 epochs.

\subsubsection{Sparse attention}
In a simple encoder-decoder model the encoder RNN usually encodes the entire input sequence to a fixed state vector. The decoder RNN then condition the predictions on the state vector. A common critique of this approach is that the fixed state vector usually has to become very large in order to encode all information for a long input sequence. Even though that architectures such as LSTMs and GRUs in theory should be able to solve such problems, in practice this is not always the case. An attention mechanism is therefore often of interest. An attention mechanism implements the ability to 'attend' to certain part of the input sequence in order to produce a single word prediction. This is often a desired feature in seq2seq models. The most common implementation uses softmax to generate the weights for which each word in the input sequence are being weighted. But since softmax never becomes zero the model will always attend to all words in the sequence. Using sparsemax as the activation function instead will solve this problem. This implies that $\boldsymbol\alpha_i=0$ in \eqref{eq:attention-model} will occur in the case of sparsemax.

The difference in attention distribution between the attention functions can be seen in figures \ref{fig:sparsemax} and \ref{fig:softmax}.

\todo[inline]{Explain the problem and the parameters of the problem we are solving.}
\begin{figure}[H]
	\centering
	\includegraphics[scale=0.5,trim={5mm 12mm 5mm 5mm},clip]{figures/attention_softmax.pdf}
	\caption{Softmax attention for an Encoder-Decoder RNN. Validation accuracy after 100k training examples: $\approx 75$\%.}
	\label{fig:softmax}
\end{figure}
\begin{figure}[H]
 	\centering
 	\includegraphics[scale=0.5,trim={5mm 12mm 5mm 5mm},clip]{figures/attention_sparsemax.pdf}
 	\caption{Sparsemax attention for an Encoder-Decoder RNN. Validation accuracy after 100k training examples: $\approx 98$\%.}
 	\label{fig:sparsemax}
\end{figure}
  
